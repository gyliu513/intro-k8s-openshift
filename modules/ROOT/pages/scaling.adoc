== Scaling
:experimental:

Navigate back to Administrator mode in the console

image::switch_to_admin_mode.png[]

Select menu:Workloads[Deployment Configs] and click the `greeter` deployment

image::deployment_config_overview.png[]

Update the replicas to **2** by clicking on the **up** arrow

image::deployment_config_edit_replicas.png[]

You should see a list with your pods similar to the following by select menu:Workloads[Pods] from the sidebar navigation of the web console.

image::deployment_config_scaled_replicas.png[]

You can see that we now have 2 replicas.

Overall, that's how simple it is to scale an application (*Pods* in a
*Service*). Application scaling can happen extremely quickly because OpenShift
is just launching new instances of an existing image, especially if that image
is already cached on the node.

== Application "Self Healing"

Because OpenShift's *DeploymentConfigs* are constantly monitoring to see that the desired number of *Pods* is actually running, you might also expect that OpenShift will fix the
situation if it is ever not right. You would be correct!

Since we have two *Pods* running right now, let's see what happens if we
"accidentally" kill one.

On the same page where you viewed the list of pods (menu:Workloads[Pods]). You will see three vertical dots (context menus) on each row of the pod replicas. Select the action __Delete__ from the context menu:

image::deployment_config_delete_pod.png[]

Click _Delete_ and confirm the dialog. You will be taken back to the page listing pods, however, this time, there are three pods.

image::deployment_config_healing.png[]

The pod that we deleted is terminating (i.e., it is being cleaned up). A new pod was created because
OpenShift will always make sure that, if one pod dies, there is going to be new pod created to
fill its place.

== Exercise: Scale Down

Before we continue, go ahead and scale your application down to a single
instance.
